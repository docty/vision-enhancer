{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init as init\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "@torch.no_grad()\n",
        "def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Args:\n",
        "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "        scale (float): Scale initialized weights, especially for residual\n",
        "            blocks. Default: 1.\n",
        "        bias_fill (float): The value to fill bias. Default: 0\n",
        "        kwargs (dict): Other arguments for initialization function.\n",
        "    \"\"\"\n",
        "    if not isinstance(module_list, list):\n",
        "        module_list = [module_list]\n",
        "    for module in module_list:\n",
        "        for m in module.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, **kwargs)\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.kaiming_normal_(m.weight, **kwargs)\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "            elif isinstance(m, _BatchNorm):\n",
        "                init.constant_(m.weight, 1)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "\n",
        "\n",
        "def make_layer(basic_block, num_basic_block, **kwarg):\n",
        "    \"\"\"Make layers by stacking the same blocks.\n",
        "\n",
        "    Args:\n",
        "        basic_block (nn.module): nn.module class for basic block.\n",
        "        num_basic_block (int): number of blocks.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for _ in range(num_basic_block):\n",
        "        layers.append(basic_block(**kwarg))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ResidualBlockNoBN(nn.Module):\n",
        "    \"\"\"Residual block without BN.\n",
        "\n",
        "    It has a style of:\n",
        "        ---Conv-ReLU-Conv-+-\n",
        "         |________________|\n",
        "\n",
        "    Args:\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "            Default: 64.\n",
        "        res_scale (float): Residual scale. Default: 1.\n",
        "        pytorch_init (bool): If set to True, use pytorch default init,\n",
        "            otherwise, use default_init_weights. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feat=64, res_scale=1, pytorch_init=False):\n",
        "        super(ResidualBlockNoBN, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.conv1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if not pytorch_init:\n",
        "            default_init_weights([self.conv1, self.conv2], 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv2(self.relu(self.conv1(x)))\n",
        "        return identity + out * self.res_scale\n",
        "\n",
        "\n",
        "class Upsample(nn.Sequential):\n",
        "    \"\"\"Upsample module.\n",
        "\n",
        "    Args:\n",
        "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale, num_feat):\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "        elif scale == 3:\n",
        "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "        else:\n",
        "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
        "        super(Upsample, self).__init__(*m)\n",
        "\n",
        "\n",
        "def flow_warp(x, flow, interp_mode='bilinear', padding_mode='zeros', align_corners=True):\n",
        "    \"\"\"Warp an image or feature map with optical flow.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Tensor with size (n, c, h, w).\n",
        "        flow (Tensor): Tensor with size (n, h, w, 2), normal value.\n",
        "        interp_mode (str): 'nearest' or 'bilinear'. Default: 'bilinear'.\n",
        "        padding_mode (str): 'zeros' or 'border' or 'reflection'.\n",
        "            Default: 'zeros'.\n",
        "        align_corners (bool): Before pytorch 1.3, the default value is\n",
        "            align_corners=True. After pytorch 1.3, the default value is\n",
        "            align_corners=False. Here, we use the True as default.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Warped image or feature map.\n",
        "    \"\"\"\n",
        "    assert x.size()[-2:] == flow.size()[1:3]\n",
        "    _, _, h, w = x.size()\n",
        "    # create mesh grid\n",
        "    grid_y, grid_x = torch.meshgrid(torch.arange(0, h).type_as(x), torch.arange(0, w).type_as(x))\n",
        "    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n",
        "    grid.requires_grad = False\n",
        "\n",
        "    vgrid = grid + flow\n",
        "    # scale grid to [-1,1]\n",
        "    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(w - 1, 1) - 1.0\n",
        "    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(h - 1, 1) - 1.0\n",
        "    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n",
        "    output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode, align_corners=align_corners)\n",
        "\n",
        "    # TODO, what if align_corners=False\n",
        "    return output\n",
        "\n",
        "\n",
        "def resize_flow(flow, size_type, sizes, interp_mode='bilinear', align_corners=False):\n",
        "    \"\"\"Resize a flow according to ratio or shape.\n",
        "\n",
        "    Args:\n",
        "        flow (Tensor): Precomputed flow. shape [N, 2, H, W].\n",
        "        size_type (str): 'ratio' or 'shape'.\n",
        "        sizes (list[int | float]): the ratio for resizing or the final output\n",
        "            shape.\n",
        "            1) The order of ratio should be [ratio_h, ratio_w]. For\n",
        "            downsampling, the ratio should be smaller than 1.0 (i.e., ratio\n",
        "            < 1.0). For upsampling, the ratio should be larger than 1.0 (i.e.,\n",
        "            ratio > 1.0).\n",
        "            2) The order of output_size should be [out_h, out_w].\n",
        "        interp_mode (str): The mode of interpolation for resizing.\n",
        "            Default: 'bilinear'.\n",
        "        align_corners (bool): Whether align corners. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Resized flow.\n",
        "    \"\"\"\n",
        "    _, _, flow_h, flow_w = flow.size()\n",
        "    if size_type == 'ratio':\n",
        "        output_h, output_w = int(flow_h * sizes[0]), int(flow_w * sizes[1])\n",
        "    elif size_type == 'shape':\n",
        "        output_h, output_w = sizes[0], sizes[1]\n",
        "    else:\n",
        "        raise ValueError(f'Size type should be ratio or shape, but got type {size_type}.')\n",
        "\n",
        "    input_flow = flow.clone()\n",
        "    ratio_h = output_h / flow_h\n",
        "    ratio_w = output_w / flow_w\n",
        "    input_flow[:, 0, :, :] *= ratio_w\n",
        "    input_flow[:, 1, :, :] *= ratio_h\n",
        "    resized_flow = F.interpolate(\n",
        "        input=input_flow, size=(output_h, output_w), mode=interp_mode, align_corners=align_corners)\n",
        "    return resized_flow\n",
        "\n",
        "\n",
        "# TODO: may write a cpp file\n",
        "def pixel_unshuffle(x, scale):\n",
        "    \"\"\" Pixel unshuffle.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input feature with shape (b, c, hh, hw).\n",
        "        scale (int): Downsample ratio.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: the pixel unshuffled feature.\n",
        "    \"\"\"\n",
        "    b, c, hh, hw = x.size()\n",
        "    out_channel = c * (scale**2)\n",
        "    assert hh % scale == 0 and hw % scale == 0\n",
        "    h = hh // scale\n",
        "    w = hw // scale\n",
        "    x_view = x.view(b, c, h, scale, w, scale)\n",
        "    return x_view.permute(0, 1, 3, 5, 2, 4).reshape(b, out_channel, h, w)"
      ],
      "metadata": {
        "id": "HFOQSxWX6d9w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GqrLAhpz6AM_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import io\n",
        "\n",
        "def pad_reflect(image, pad_size):\n",
        "    imsize = image.shape\n",
        "    height, width = imsize[:2]\n",
        "    new_img = np.zeros([height+pad_size*2, width+pad_size*2, imsize[2]]).astype(np.uint8)\n",
        "    new_img[pad_size:-pad_size, pad_size:-pad_size, :] = image\n",
        "\n",
        "    new_img[0:pad_size, pad_size:-pad_size, :] = np.flip(image[0:pad_size, :, :], axis=0) #top\n",
        "    new_img[-pad_size:, pad_size:-pad_size, :] = np.flip(image[-pad_size:, :, :], axis=0) #bottom\n",
        "    new_img[:, 0:pad_size, :] = np.flip(new_img[:, pad_size:pad_size*2, :], axis=1) #left\n",
        "    new_img[:, -pad_size:, :] = np.flip(new_img[:, -pad_size*2:-pad_size, :], axis=1) #right\n",
        "\n",
        "    return new_img\n",
        "\n",
        "def unpad_image(image, pad_size):\n",
        "    return image[pad_size:-pad_size, pad_size:-pad_size, :]\n",
        "\n",
        "\n",
        "def process_array(image_array, expand=True):\n",
        "    \"\"\" Process a 3-dimensional array into a scaled, 4 dimensional batch of size 1. \"\"\"\n",
        "\n",
        "    image_batch = image_array / 255.0\n",
        "    if expand:\n",
        "        image_batch = np.expand_dims(image_batch, axis=0)\n",
        "    return image_batch\n",
        "\n",
        "\n",
        "def process_output(output_tensor):\n",
        "    \"\"\" Transforms the 4-dimensional output tensor into a suitable image format. \"\"\"\n",
        "\n",
        "    sr_img = output_tensor.clip(0, 1) * 255\n",
        "    sr_img = np.uint8(sr_img)\n",
        "    return sr_img\n",
        "\n",
        "\n",
        "def pad_patch(image_patch, padding_size, channel_last=True):\n",
        "    \"\"\" Pads image_patch with with padding_size edge values. \"\"\"\n",
        "\n",
        "    if channel_last:\n",
        "        return np.pad(\n",
        "            image_patch,\n",
        "            ((padding_size, padding_size), (padding_size, padding_size), (0, 0)),\n",
        "            'edge',\n",
        "        )\n",
        "    else:\n",
        "        return np.pad(\n",
        "            image_patch,\n",
        "            ((0, 0), (padding_size, padding_size), (padding_size, padding_size)),\n",
        "            'edge',\n",
        "        )\n",
        "\n",
        "\n",
        "def unpad_patches(image_patches, padding_size):\n",
        "    return image_patches[:, padding_size:-padding_size, padding_size:-padding_size, :]\n",
        "\n",
        "\n",
        "def split_image_into_overlapping_patches(image_array, patch_size, padding_size=2):\n",
        "    \"\"\" Splits the image into partially overlapping patches.\n",
        "    The patches overlap by padding_size pixels.\n",
        "    Pads the image twice:\n",
        "        - first to have a size multiple of the patch size,\n",
        "        - then to have equal padding at the borders.\n",
        "    Args:\n",
        "        image_array: numpy array of the input image.\n",
        "        patch_size: size of the patches from the original image (without padding).\n",
        "        padding_size: size of the overlapping area.\n",
        "    \"\"\"\n",
        "\n",
        "    xmax, ymax, _ = image_array.shape\n",
        "    x_remainder = xmax % patch_size\n",
        "    y_remainder = ymax % patch_size\n",
        "\n",
        "    # modulo here is to avoid extending of patch_size instead of 0\n",
        "    x_extend = (patch_size - x_remainder) % patch_size\n",
        "    y_extend = (patch_size - y_remainder) % patch_size\n",
        "\n",
        "    # make sure the image is divisible into regular patches\n",
        "    extended_image = np.pad(image_array, ((0, x_extend), (0, y_extend), (0, 0)), 'edge')\n",
        "\n",
        "    # add padding around the image to simplify computations\n",
        "    padded_image = pad_patch(extended_image, padding_size, channel_last=True)\n",
        "\n",
        "    xmax, ymax, _ = padded_image.shape\n",
        "    patches = []\n",
        "\n",
        "    x_lefts = range(padding_size, xmax - padding_size, patch_size)\n",
        "    y_tops = range(padding_size, ymax - padding_size, patch_size)\n",
        "\n",
        "    for x in x_lefts:\n",
        "        for y in y_tops:\n",
        "            x_left = x - padding_size\n",
        "            y_top = y - padding_size\n",
        "            x_right = x + patch_size + padding_size\n",
        "            y_bottom = y + patch_size + padding_size\n",
        "            patch = padded_image[x_left:x_right, y_top:y_bottom, :]\n",
        "            patches.append(patch)\n",
        "\n",
        "    return np.array(patches), padded_image.shape\n",
        "\n",
        "\n",
        "def stich_together(patches, padded_image_shape, target_shape, padding_size=4):\n",
        "    \"\"\" Reconstruct the image from overlapping patches.\n",
        "    After scaling, shapes and padding should be scaled too.\n",
        "    Args:\n",
        "        patches: patches obtained with split_image_into_overlapping_patches\n",
        "        padded_image_shape: shape of the padded image contructed in split_image_into_overlapping_patches\n",
        "        target_shape: shape of the final image\n",
        "        padding_size: size of the overlapping area.\n",
        "    \"\"\"\n",
        "\n",
        "    xmax, ymax, _ = padded_image_shape\n",
        "    patches = unpad_patches(patches, padding_size)\n",
        "    patch_size = patches.shape[1]\n",
        "    n_patches_per_row = ymax // patch_size\n",
        "\n",
        "    complete_image = np.zeros((xmax, ymax, 3))\n",
        "\n",
        "    row = -1\n",
        "    col = 0\n",
        "    for i in range(len(patches)):\n",
        "        if i % n_patches_per_row == 0:\n",
        "            row += 1\n",
        "            col = 0\n",
        "        complete_image[\n",
        "        row * patch_size: (row + 1) * patch_size, col * patch_size: (col + 1) * patch_size,:\n",
        "        ] = patches[i]\n",
        "        col += 1\n",
        "    return complete_image[0: target_shape[0], 0: target_shape[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#from .arch_utils import default_init_weights, make_layer, pixel_unshuffle\n",
        "\n",
        "\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    \"\"\"Residual Dense Block.\n",
        "\n",
        "    Used in RRDB block in ESRGAN.\n",
        "\n",
        "    Args:\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "        num_grow_ch (int): Channels for each growth.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feat=64, num_grow_ch=32):\n",
        "        super(ResidualDenseBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_feat, num_grow_ch, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(num_feat + num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(num_feat + 2 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv4 = nn.Conv2d(num_feat + 3 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv5 = nn.Conv2d(num_feat + 4 * num_grow_ch, num_feat, 3, 1, 1)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        # initialization\n",
        "        default_init_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lrelu(self.conv1(x))\n",
        "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
        "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
        "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        # Emperically, we use 0.2 to scale the residual for better performance\n",
        "        return x5 * 0.2 + x\n",
        "\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    \"\"\"Residual in Residual Dense Block.\n",
        "\n",
        "    Used in RRDB-Net in ESRGAN.\n",
        "\n",
        "    Args:\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "        num_grow_ch (int): Channels for each growth.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feat, num_grow_ch=32):\n",
        "        super(RRDB, self).__init__()\n",
        "        self.rdb1 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "        self.rdb2 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "        self.rdb3 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.rdb1(x)\n",
        "        out = self.rdb2(out)\n",
        "        out = self.rdb3(out)\n",
        "        # Emperically, we use 0.2 to scale the residual for better performance\n",
        "        return out * 0.2 + x\n",
        "\n",
        "\n",
        "class RRDBNet(nn.Module):\n",
        "    \"\"\"Networks consisting of Residual in Residual Dense Block, which is used\n",
        "    in ESRGAN.\n",
        "\n",
        "    ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks.\n",
        "\n",
        "    We extend ESRGAN for scale x2 and scale x1.\n",
        "    Note: This is one option for scale 1, scale 2 in RRDBNet.\n",
        "    We first employ the pixel-unshuffle (an inverse operation of pixelshuffle to reduce the spatial size\n",
        "    and enlarge the channel size before feeding inputs into the main ESRGAN architecture.\n",
        "\n",
        "    Args:\n",
        "        num_in_ch (int): Channel number of inputs.\n",
        "        num_out_ch (int): Channel number of outputs.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "            Default: 64\n",
        "        num_block (int): Block number in the trunk network. Defaults: 23\n",
        "        num_grow_ch (int): Channels for each growth. Default: 32.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_in_ch, num_out_ch, scale=4, num_feat=64, num_block=23, num_grow_ch=32):\n",
        "        super(RRDBNet, self).__init__()\n",
        "        self.scale = scale\n",
        "        if scale == 2:\n",
        "            num_in_ch = num_in_ch * 4\n",
        "        elif scale == 1:\n",
        "            num_in_ch = num_in_ch * 16\n",
        "        self.conv_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)\n",
        "        self.body = make_layer(RRDB, num_block, num_feat=num_feat, num_grow_ch=num_grow_ch)\n",
        "        self.conv_body = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "        # upsample\n",
        "        self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "        self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "        if scale == 8:\n",
        "            self.conv_up3 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "        self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "        self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.scale == 2:\n",
        "            feat = pixel_unshuffle(x, scale=2)\n",
        "        elif self.scale == 1:\n",
        "            feat = pixel_unshuffle(x, scale=4)\n",
        "        else:\n",
        "            feat = x\n",
        "        feat = self.conv_first(feat)\n",
        "        body_feat = self.conv_body(self.body(feat))\n",
        "        feat = feat + body_feat\n",
        "        # upsample\n",
        "        feat = self.lrelu(self.conv_up1(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
        "        feat = self.lrelu(self.conv_up2(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
        "        if self.scale == 8:\n",
        "            feat = self.lrelu(self.conv_up3(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
        "        out = self.conv_last(self.lrelu(self.conv_hr(feat)))\n",
        "        return out"
      ],
      "metadata": {
        "id": "TF7VspDr8tJv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from huggingface_hub import hf_hub_url\n",
        "\n",
        "#from .rrdbnet_arch import RRDBNet\n",
        "#from .utils import pad_reflect, split_image_into_overlapping_patches, stich_together, \\\n",
        "                   #unpad_image\n",
        "\n",
        "\n",
        "HF_MODELS = {\n",
        "    2: dict(\n",
        "        repo_id='sberbank-ai/Real-ESRGAN',\n",
        "        filename='RealESRGAN_x2.pth',\n",
        "    ),\n",
        "    4: dict(\n",
        "        repo_id='sberbank-ai/Real-ESRGAN',\n",
        "        filename='RealESRGAN_x4.pth',\n",
        "    ),\n",
        "    8: dict(\n",
        "        repo_id='sberbank-ai/Real-ESRGAN',\n",
        "        filename='RealESRGAN_x8.pth',\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "class RealESRGAN:\n",
        "    def __init__(self, device, scale=4):\n",
        "        self.device = device\n",
        "        self.scale = scale\n",
        "        self.model = RRDBNet(\n",
        "            num_in_ch=3, num_out_ch=3, num_feat=64,\n",
        "            num_block=23, num_grow_ch=32, scale=scale\n",
        "        )\n",
        "\n",
        "    def load_weights(self, model_path, download=True):\n",
        "        if not os.path.exists(model_path) and download:\n",
        "            print('Good morining')\n",
        "            #assert self.scale in [2,4,8], 'You can download models only with scales: 2, 4, 8'\n",
        "            #config = HF_MODELS[self.scale]\n",
        "            #cache_dir = os.path.dirname(model_path)\n",
        "            #local_filename = os.path.basename(model_path)\n",
        "            #config_file_url = hf_hub_url(repo_id=config['repo_id'], filename=config['filename'])\n",
        "            #cached_download(config_file_url, cache_dir=cache_dir, force_filename=local_filename)\n",
        "            #print('Weights downloaded to:', os.path.join(cache_dir, local_filename))\n",
        "\n",
        "        loadnet = torch.load(model_path)\n",
        "        if 'params' in loadnet:\n",
        "            self.model.load_state_dict(loadnet['params'], strict=True)\n",
        "        elif 'params_ema' in loadnet:\n",
        "            self.model.load_state_dict(loadnet['params_ema'], strict=True)\n",
        "        else:\n",
        "            self.model.load_state_dict(loadnet, strict=True)\n",
        "        self.model.eval()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    @torch.cuda.amp.autocast()\n",
        "    def predict(self, lr_image, batch_size=4, patches_size=192,\n",
        "                padding=24, pad_size=15):\n",
        "        scale = self.scale\n",
        "        device = self.device\n",
        "        lr_image = np.array(lr_image)\n",
        "        lr_image = pad_reflect(lr_image, pad_size)\n",
        "\n",
        "        patches, p_shape = split_image_into_overlapping_patches(\n",
        "            lr_image, patch_size=patches_size, padding_size=padding\n",
        "        )\n",
        "        img = torch.FloatTensor(patches/255).permute((0,3,1,2)).to(device).detach()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            res = self.model(img[0:batch_size])\n",
        "            for i in range(batch_size, img.shape[0], batch_size):\n",
        "                res = torch.cat((res, self.model(img[i:i+batch_size])), 0)\n",
        "\n",
        "        sr_image = res.permute((0,2,3,1)).clamp_(0, 1).cpu()\n",
        "        np_sr_image = sr_image.numpy()\n",
        "\n",
        "        padded_size_scaled = tuple(np.multiply(p_shape[0:2], scale)) + (3,)\n",
        "        scaled_image_shape = tuple(np.multiply(lr_image.shape[0:2], scale)) + (3,)\n",
        "        np_sr_image = stich_together(\n",
        "            np_sr_image, padded_image_shape=padded_size_scaled,\n",
        "            target_shape=scaled_image_shape, padding_size=padding * scale\n",
        "        )\n",
        "        sr_img = (np_sr_image*255).astype(np.uint8)\n",
        "        sr_img = unpad_image(sr_img, pad_size*scale)\n",
        "        sr_img = Image.fromarray(sr_img)\n",
        "\n",
        "        return sr_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYJaElJ888Cr",
        "outputId": "d34f8a5c-9827-4e04-a048-46fd85d588ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-f77e82d128ec>:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @torch.cuda.amp.autocast()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "#from RealESRGAN import RealESRGAN\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = RealESRGAN(device, scale=4)\n",
        "    model.load_weights('/content/weights/RealESRGAN_x4.pth', download=True)\n",
        "    for i, image in enumerate(os.listdir(\"inputs\")):\n",
        "        image = Image.open(f\"inputs/{image}\").convert('RGB')\n",
        "        sr_image = model.predict(image)\n",
        "        sr_image.save(f'results/{i}.png')\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "xUT6S2eJ9Jl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}